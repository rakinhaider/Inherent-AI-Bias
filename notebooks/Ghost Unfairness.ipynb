{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../\")\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    import inherent_bias\n",
    "    importlib.reload(inherent_bias.fair_dataset)\n",
    "    importlib.reload(inherent_bias.utils)\n",
    "    \n",
    "reload_modules()\n",
    "\n",
    "from inherent_bias.fair_dataset import FairDataset\n",
    "\n",
    "from inherent_bias.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber that after the data is processed, all privileged class values are mapped to 1 and all unprivileged class values are mapped to 0. That means in the following case, after the processing, \"Male\" will be mapped to 1 and \"Female\" will be mapped to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = [\"sex\"]\n",
    "privileged_classes = [['Male']]\n",
    "\n",
    "\n",
    "privileged_groups = [{key:1 for key in protected}]\n",
    "unprivileged_groups = [{key:0 for key in protected}]\n",
    "\n",
    "random_state = 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'n_redlin'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6516\\2167238900.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m fd_train = FairDataset(10000, 5, \n\u001B[0m\u001B[0;32m      2\u001B[0m                       \u001B[0mprotected_attribute_names\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'sex'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m                       \u001B[0mprivileged_classes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Male'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m                       random_state=random_state)\n\u001B[0;32m      5\u001B[0m fd_test = FairDataset(5000, 5,\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 1 required positional argument: 'n_redlin'"
     ]
    }
   ],
   "source": [
    "fd_train = FairDataset(10000, 5, \n",
    "                      protected_attribute_names=['sex'],\n",
    "                      privileged_classes=[['Male']],\n",
    "                      random_state=random_state)\n",
    "fd_test = FairDataset(5000, 5,\n",
    "                      protected_attribute_names=['sex'],\n",
    "                      privileged_classes=[['Male']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_train_x, fd_train_y = fd_train.get_xy(keep_protected = False)\n",
    "fd_test_x, fd_test_y = fd_test.get_xy(keep_protected = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_metrics(fd_train,\n",
    "                    unprivileged_groups,\n",
    "                    privileged_groups, \n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmod = LogisticRegression(class_weight='balanced', \n",
    "                          solver='liblinear',\n",
    "                         verbose=2)\n",
    "lmod.fit(fd_train_x, fd_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classifier_metrics(lmod, fd_test,\n",
    "                       privileged_groups, \n",
    "                       unprivileged_groups, \n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmod = DecisionTreeClassifier(criterion='entropy', \n",
    "                                   max_depth=40,\n",
    "                                  random_state=47)\n",
    "dmod = dmod.fit(fd_train_x, fd_train_y)\n",
    "\n",
    "get_classifier_metrics(dmod, fd_test, \n",
    "                       privileged_groups, \n",
    "                       unprivileged_groups, \n",
    "                       verbose=True)\n",
    "\n",
    "print('Tree Depth:', dmod.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation of fairness with model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_variants(model_type, fd, variant, values,\n",
    "                        params):\n",
    "    fd_x, fd_y = fd.get_xy(keep_protected=False)\n",
    "    models = []\n",
    "    for val in values:\n",
    "        model = model_type()\n",
    "        params[variant] = val\n",
    "        model.set_params(**params)\n",
    "\n",
    "        model = model.fit(fd_x, fd_y)\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(model, train_fd, test_fd, func=None):\n",
    "    if func:\n",
    "        model_property = func(model)\n",
    "    else:\n",
    "        model_property = None\n",
    "    \n",
    "    md, di, ac = get_classifier_metrics(model, train_fd,\n",
    "                                        privileged_groups, \n",
    "                                        unprivileged_groups, \n",
    "                                        verbose=True)\n",
    "    train_result = (md, di, ac, model_property)\n",
    "    \n",
    "    print('Test')\n",
    "    \n",
    "    md, di, ac = get_classifier_metrics(model, test_fd,\n",
    "                                        privileged_groups, \n",
    "                                        unprivileged_groups,\n",
    "                                       verbose=True)\n",
    "    test_result = (md, di, ac, model_property)\n",
    "    \n",
    "    return train_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(models, train_fd, test_fd, func=None):\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for model in models:\n",
    "        train_result, test_result = get_model_results(model, train_fd, \n",
    "                                                      test_fd, func)\n",
    "        train_results.append(train_result)\n",
    "        test_results.append(test_result)\n",
    "        \n",
    "    return train_results, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_property(model):\n",
    "    return {'depth': model.get_depth()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [3, 5, 10, 15, 20, 25]\n",
    "\n",
    "params = {'criterion':'entropy',\n",
    "          'random_state': 47}\n",
    "variant = 'max_depth'\n",
    "dmods = train_model_variants(DecisionTreeClassifier,\n",
    "                            fd_train, variant, max_depths,\n",
    "                            params)\n",
    "\n",
    "train_results, test_results = get_results(dmods, fd_train, fd_test,\n",
    "                                         decision_tree_property)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing model complexity tends to increase the disparate impact and mean difference at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_vs_metric(results, plot_type='line'):\n",
    "    mds = [abs(tup[0]) for tup in results]\n",
    "    dis = [abs(1 - tup[1]) for tup in results]\n",
    "    acs = [tup[2] for tup in results]\n",
    "    tds = [tup[3] for tup in results]\n",
    "    \n",
    "    if plot_type == 'line':\n",
    "        plt.plot(acs, mds, '*-', label='Mean Difference')\n",
    "        plt.plot(acs, dis, '*-', label='Disparate Impact')\n",
    "    elif plot_type == 'scatter':\n",
    "        plt.scatter(acs, mds, label='Mean Difference')\n",
    "        plt.scatter(acs, dis, label='Disparate Impact')        \n",
    "    plt.legend()\n",
    "    \n",
    "plot_acc_vs_metric(train_results)\n",
    "plt.show()\n",
    "plot_acc_vs_metric(test_results, 'scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same characteristic can also be seen in the test dataset as well. Trying to increase accuracy in the test dataset also increases the disparate impact and mean difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_complexity_vs_metric(results, params):\n",
    "    mds = [abs(tup[0]) for tup in results]\n",
    "    dis = [abs(1-tup[1]) for tup in results]\n",
    "    acs = [tup[2] for tup in results]\n",
    "    tds = [tup[3] for tup in results]\n",
    "\n",
    "    plt.plot(params, mds, '*-', label='Mean Difference')\n",
    "    plt.plot(params, dis, '*-', label='Disparate Impact')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "plot_complexity_vs_metric(train_results, max_depths)\n",
    "plt.show()\n",
    "plot_complexity_vs_metric(test_results, max_depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see group wise accuracy rates for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_accuracy(model, train_fd):\n",
    "    train_fd_p = train_fd.get_privileged_group()\n",
    "    train_fd_u = train_fd.get_unprivileged_group()\n",
    "    \n",
    "    _, _, acc_u = get_classifier_metrics(model, train_fd_p,\n",
    "                        privileged_groups=privileged_groups,\n",
    "                        unprivileged_groups=unprivileged_groups)\n",
    "    \n",
    "    _, _, acc_p = get_classifier_metrics(model, train_fd_u,\n",
    "                        privileged_groups=privileged_groups,\n",
    "                        unprivileged_groups=unprivileged_groups)\n",
    "    return acc_p, acc_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_differences(models, train_fd):\n",
    "    acc_diffs = []\n",
    "    for model in models:\n",
    "        acc_p, acc_u = get_group_accuracy(model, train_fd)\n",
    "        acc_diffs.append(acc_p - acc_u)\n",
    "        \n",
    "    return acc_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "acc_diffs = get_accuracy_differences(dmods, fd_train)\n",
    "print(acc_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy difference between privileged and unprivileged groups on the training dataset shows similar trend to the training dataset disparate impact.\n",
    "\n",
    "Therefore, we want to claim that while training, one group is picked over the other as a winner even though they have same ratio of positive and negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "def plot_accuracy_diffrences(results, acc_diffs, values):\n",
    "    dis = [abs(1 - tup[1]) for tup in results] \n",
    "    plt.plot(values, acc_diffs, '*-', label='Accuracy Difference')\n",
    "    plt.plot(values, dis, '*-', label='Disparate Impact')\n",
    "    plt.legend()\n",
    "    \n",
    "acc_diffs = get_accuracy_differences(dmods, fd_train)\n",
    "plot_accuracy_diffrences(train_results, acc_diffs, max_depths)\n",
    "plt.show()\n",
    "print(train_results)\n",
    "print(acc_diffs)\n",
    "acc_diffs = get_accuracy_differences(dmods, fd_test)\n",
    "plot_accuracy_diffrences(test_results, acc_diffs, max_depths)\n",
    "plt.show()\n",
    "print(test_results)\n",
    "print(acc_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "group_accs = [get_group_accuracy(model, fd_train) for model in dmods]\n",
    "acs_p = [tup[0] for tup in group_accs]\n",
    "acs_u = [tup[1] for tup in group_accs]\n",
    "plt.plot(max_depths, acs_p, '*-', label='Privileged')\n",
    "plt.plot(max_depths, acs_u, '*-', label='Un-privileged')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with reqularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg_property(model):\n",
    "    # TODO: will return theta from this function.\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizers = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "\n",
    "params = {'class_weight': 'balanced',\n",
    "          'solver': 'liblinear'}\n",
    "variant = 'C'\n",
    "lmods = train_model_variants(LogisticRegression,\n",
    "                            fd_train, variant, regularizers,\n",
    "                            params)\n",
    "\n",
    "lr_train_results, lr_test_results = get_results(lmods, fd_train, fd_test,\n",
    "                                                logistic_reg_property)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr_train_results)\n",
    "lr_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_vs_metric(lr_train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_complexity_vs_metric(lr_test_results, np.log10(regularizers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "acc_diffs = get_accuracy_differences(lmods, fd_train)\n",
    "plot_accuracy_diffrences(lr_train_results, acc_diffs, np.log10(regularizers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "acc_diffs = get_accuracy_differences(lmods, fd_test)\n",
    "plot_accuracy_diffrences(lr_test_results, acc_diffs, np.log10(regularizers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "group_accs = [get_group_accuracy(model, fd_train) for model in lmods]\n",
    "acs_p = [tup[0] for tup in group_accs]\n",
    "acs_u = [tup[1] for tup in group_accs]\n",
    "plt.plot(np.log10(regularizers), acs_p, '*-', label='Privileged')\n",
    "plt.plot(np.log10(regularizers), acs_u, '*-', label='Un-privileged')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does test measures average out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fds = []\n",
    "for i in range(10):\n",
    "    temp_fd = FairDataset(200, 5,\n",
    "                          protected_attribute_names=['sex'],\n",
    "                          privileged_classes=[['Male']],\n",
    "                          random_state=i)\n",
    "    test_fds.append(temp_fd)\n",
    "    # print(temp_fd)\n",
    "    \n",
    "    \n",
    "results = []\n",
    "for i in range(len(lmods)):\n",
    "    results.append([])\n",
    "    for fd in test_fds:\n",
    "        md, di, acc = get_classifier_metrics(lmods[i], fd,\n",
    "                                             privileged_groups=privileged_groups,\n",
    "                                             unprivileged_groups=unprivileged_groups)\n",
    "               \n",
    "        \n",
    "        results[i] += [[abs(md), abs(1-di), acc]]\n",
    "    \n",
    "    # print(results[i])\n",
    "\n",
    "    \n",
    "results = np.array(results)\n",
    "means = np.mean(results, axis=1)\n",
    "variations = np.std(results, axis=1)\n",
    "# print(results)\n",
    "print(means)\n",
    "print(variations)\n",
    "plt.errorbar(np.log10(regularizers), means[:, 1], yerr=variations[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(fd_train_x, fd_train_y)\n",
    "\n",
    "gnb_results = []\n",
    "for fd in test_fds:\n",
    "    gnb_results.append(get_classifier_metrics(gnb, fd, \n",
    "                                 privileged_groups, \n",
    "                                 unprivileged_groups))\n",
    "    print(gnb_results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Standard Deviation for Bayes classifier Disparate Impact:')\n",
    "print(np.std([abs(1-tup[1])for tup in gnb_results]))\n",
    "print(np.mean([abs(1-tup[1])for tup in gnb_results]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (GhostUnfairness)",
   "language": "python",
   "name": "pycharm-fd9c5fd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}